## 分布式系统
三大基础：
1. 存储
2. 通信
3. 计算

三大工具：
1. RPC（remote procedure）  :远程过程通信，掩盖在不可靠网络上通信的事实）
2. threads          :线程，利用计算机的多核心
3. concurrency ctl  :并发控制，比如锁

## 几个重要的话题
Performance 性能表现：
**Scalability** 可扩展能力 :用多个计算机实现相同时间计算更多的问题，数倍地提升计算性能

**Fault Tolerance** 容错表现: 在不可靠的机器上，运行可靠的程序。错误总会发生，需要掩盖、处理这种错误
**Availability** 高可用 : 提供完整的服务，就像没有发生错误，生成多个备份，一个备份出错了，可以用另一个备份
**Recoverability** 可恢复性：需要被修复用以继续可用而不是停止响应
**NV storage** (non-volatile storage) 非易失性存储：在故障发生之后没比如电源故障，用非易失性存储比如硬盘、闪存、SSD之类的存储工具，存放一些检查点或者关于系统状态的日志，在故障恢复之后可用通知读取最新的硬盘状态，并且继续从那里操作。
**Replication** 复制，主从同步性

consistency 一致性
例如 K-V 数据库是一个分布式系统，系统中保持一份数据的多个副本，要保持数据的一致性
Set(k,v)
get(k,v)

## MapReduce
论文研读：
https://hardcore.feishu.cn/docs/doccnxwr1i2y3Ak3WXmFlWLaCbh
MapReduce 是为了运行大规模并行数据处理程序而抽象出来的编程模型。
为了解决**多机并行协同，网络通信，处理错误，提高执行效率**
可用于：大规模词频统计、分布式排序、url访问频次统计、倒排索引
MapReduce 的实现：
1. （分片）用户程序中的MapReduce库首先将输入文件切分为M块，每块的大小从16MB到64MB（用户可通过一个可选参数控制此大小）。然后MapReduce库会在一个集群的若干台机器上启动程序的多个副本。
2. （下发）程序的各个副本中有一个是特殊的——主节点，其它的则是工作节点。主节点将M个map任务和R个reduce任务分配给空闲的工作节点，每个节点一项任务。
3. 被分配map任务的工作节点读取对应的输入区块内容。它从输入数据中解析出key/value对，然后将每个对传递给用户定义的map函数。由map函数产生的中间key/value对都缓存在内存中。
4. 缓存的数据对会被周期性的由划分函数分成R块，并写入本地磁盘中。这些缓存对在本地磁盘中的位置会被传回给主节点，主节点负责将这些位置再传给reduce工作节点。
5. 当一个reduce工作节点得到了主节点的这些位置通知后，它使用RPC调用去读map工作节点的本地磁盘中的缓存数据。当reduce工作节点读取完了所有的中间数据，它会将这些数据按中间key排序，这样相同key的数据就被排列在一起了。同一个reduce任务经常会分到有着不同key的数据，因此这个排序很有必要。如果中间数据数量过多，不能全部载入内存，则会使用外部排序。
6. reduce工作节点遍历排序好的中间数据，并将遇到的每个中间key和与它关联的一组中间value传递给用户的reduce函数。reduce函数的输出会写到由reduce划分过程划分出来的最终输出文件的末尾。
7. 当所有的map和reduce任务都完成后，主节点唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码中。
8. 主节点维持多种数据结构。它会存储每个map和reduce任务的状态（空闲、处理中、完成），和每台工作机器的ID（对应非空闲的任务）。
9. 主节点是将map任务产生的中间文件的位置传递给reduce任务的通道。因此，主节点要存储每个已完成的map任务产生的R个中间文件的位置和大小。位置和大小信息的更新情况会在map任务完成时接收到。这些信息会被逐步发送到正在处理中的reduce任务节点处。

## Shuffle 过程
因为频繁的磁盘I/O操作会严重的降低效率，因此“中间结果”不会立马写入磁盘，而是优先存储到map节点的**环形内存缓冲区**，在写入的过程中进行分区（partition），也就是对于每个键值对来说，都增加了一个partition属性值，然后连同键值对一起序列化成字节数组写入到缓冲区（缓冲区采用的就是字节数组，默认大小为100M）。当写入的数据量**达到预先设置的阙值**后（mapreduce.map.io.sort.spill.percent,默认0.80，或者80%）便会**启动溢出写线程**将缓冲区中的那部分数据溢出写（spill）到磁盘的临时文件中，并在写入前根据key进行排序（sort）和合并（combine，可选操作）。溢出写过程按**轮询方式**将缓冲区中的内容写到mapreduce.cluster.local.dir属性指定的目录中。当整个map任务完成溢出写后，会对磁盘中这个map任务产生的所有临时文件（spill文件）进行归并（merge）操作生成最终的正式输出文件，此时的归并是将所有spill文件中的相同partition合并到一起，并对各个partition中的数据再进行一次排序（sort），生成key和对应的value-list，文件归并时，如果溢写文件数量超过参数min.num.spills.for.combine的值（默认为3）时，可以再次进行合并。
至此，map端shuffle过程结束，接下来等待reduce task来拉取数据。对于reduce端的shuffle过程来说，reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge最后合并成一个分区相同的大文件，然后对这个文件中的键值对按照key进行sort排序，排好序之后紧接着进行分组，分组完成后才将整个文件交给reduce task处理。


## MapReduce 会存在的问题以及改进

### 节点的容错
**map任务 worker**：心跳机制，master 会对 worker 节点心跳检查，心跳超时，认为节点失败，把 task 分配其他的 worker。主节点维护**任务队列**，记录每个节点的状态（已完成、未完成、处理中、失败等）和每个 worker 对应的 id
**reduce任务 worker**：reduce 产出的文件是结果文件，所以出错重启后会写入一个临时文件，对其重命名覆盖原先文件，防止和损坏文件冲突
**master**：对于任务执行产生的中间文件，定时保存在一个恢复点文件中，节点崩溃重启后，选择最近的恢复点重新执行 mapreduce 任务
**副作用**：对于持久化操作都有副作用，崩溃后重启会写入一个临时文件，完成后重命名覆盖原文件，防止和损坏文件冲突

## 加速并行
利用局部性：网络带宽瓶颈，master 分发任务给本来就有所有输入文件的节点，减少网络调用
任务的粒度：map和reduce任务的数量，map任务通常时输入文件总大小除以64M的值（底层分布式文件系统是以64m为一个chuck存储），reduce的任务数量通常是map任务的一半
备用任务：最后几个任务时间过长，启用备用任务，同时在两个节点上执行相同任务，只要其中一个先返回即可结束整个任务，同时释放未完成的任务所占用的资源

